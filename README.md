# California Housing - Machine Learning Project

## Authors
[МУханбеткерей Каримов]

## Project Overview
**Реализация алгоритмов машинного обучения с нуля** на основе набора данных California Housing. Этот проект демонстрирует:
- **Реализацию градиентного спуска** для линейной и логистической регрессии
- **Сравнение моделей** и их оценку
- **Интерактивный интерфейс** для настройки гиперпараметров

## Описание данных

### Источник данных
- **Набор данных:** California Housing Dataset
- **Источник:** Перепись населения Калифорнии 1990 года
- **Используется в:** Scikit-learn
- **Образцы:** 20,640 записей

### Переменные (признаки)
1. **longitude, latitude:** Географические координаты
2. **housing_median_age:** Медианный возраст домов
3. **total_rooms:** Общее количество комнат
4. **total_bedrooms:** Общее количество спален
5. **population:** Население в районе
6. **households:** Количество домохозяйств
7. **median_income:** Медианный доход (масштабированный)
8. **median_house_value:** Целевая переменная (стоимость дома в $)
9. **ocean_proximity:** Категориальный признак (близость к океану)

## Что реализовано в проекте

### Часть 1: Линейная регрессия с градиентным спуском
- **Алгоритм градиентного спуска** реализован с нуля
- **Функция потерь:** MSE (среднеквадратичная ошибка)
- **Ручной расчет градиентов:**
  - ∇w = (2/n) * Xᵀ(y_pred - y)
  - ∇b = (2/n) * Σ(y_pred - y)
- **Поддержка мини-батчей** для ускорения обучения
- **Визуализация** процесса обучения и сходимости

### Часть 2: Логистическая регрессия с градиентным спуском
- **Бинарная классификация** (дорогие/дешевые дома)
- **Создание целевой переменной:** HighPrice = median_house_value > медиана
- **Сигмоидная функция активации**
- **Функция потерь:** Log Loss с L2 регуляризацией
- **Градиенты:**
  - ∇w = (1/n) * Xᵀ(y_pred - y) + (λ/n) * w
  - ∇b = (1/n) * Σ(y_pred - y)

### Часть 3: Сравнение моделей
- **Кастомная логистическая регрессия** vs **Random Forest**
- **Метрики оценки:**
  - Accuracy (точность)
  - Precision (точность положительных прогнозов)
  - Recall (полнота)
  - F1-Score (гармоническое среднее)
  - ROC AUC (площадь под ROC-кривой)
- **Визуализация:** Confusion matrix, ROC-кривая

### Часть 4: Интерактивный интерфейс
- **Настройка гиперпараметров** в реальном времени
- **Визуализация** обучения модели
- **Сравнение** train/test производительности
- **Анализ** коэффициентов модели

## Установка и запуск

### Вариант 1: Google Colab (рекомендуется)
1. Откройте [Google Colab](https://colab.research.google.com/)
2. Создайте новый ноутбук
3. Скопируйте код из проекта
4. Запустите все ячейки (Runtime → Run all)

### Вариант 2: Локальная установка

```bash
# 1. Установите зависимости
pip install numpy pandas matplotlib seaborn scikit-learn ipywidgets tqdm

# 2. Загрузите данные
# Файл housing.csv должен быть в рабочей директории

# 3. Запустите код в Jupyter Notebook или Python скрипте
```

## Требования
- Python 3.7+
- numpy>=1.21.0
- pandas>=1.3.0
- matplotlib>=3.4.0
- seaborn>=0.11.0
- scikit-learn>=0.24.0
- ipywidgets>=7.6.0
- tqdm>=4.62.0

## Реализация алгоритмов

### Класс LinearRegressionGD
```python
class LinearRegressionGD:
    def fit(self, X, y):
        # Градиент: ∇J(w) = (1/m) * X^T * (Xw - y)
        # Обновление: w = w - α * ∇J(w)
        # Функция потерь: J(w) = (1/2m) * Σ(Xw - y)^2
```

### Класс LogisticRegressionGD
```python
class LogisticRegressionGD:
    def fit(self, X, y):
        # Градиент: ∇J(w) = (1/m) * X^T * (σ(Xw) - y) + (λ/m) * w
        # Обновление: w = w - α * ∇J(w)
        # Функция потерь: J(w) = -(1/m) * Σ[y*log(σ(Xw)) + (1-y)*log(1-σ(Xw))]
```

## Результаты

### Линейная регрессия
- **Training MSE:** ~0.2541
- **Test MSE:** ~0.2478
- **R² Score:** ~0.65
- **Вывод:** Модель хорошо обобщается (test MSE < train MSE)

### Логистическая регрессия
- **Accuracy:** 82.0%
- **Precision:** 81.0%
- **Recall:** 74.0%
- **F1-Score:** 77.0%
- **ROC AUC:** 90.0%
- **Вывод:** Хорошая классификация с сбалансированными метриками

### Random Forest (для сравнения)
- **Accuracy:** 83.5%
- **Precision:** 82.1%
- **Recall:** 75.8%
- **F1-Score:** 78.8%

## Визуализации

1. **График функции потерь** по эпохам
2. **Scatter plot** предсказаний vs реальных значений
3. **Коэффициенты модели** (важность признаков)
4. **Confusion matrix** для классификации
5. **ROC-кривая** и AUC
6. **Сравнение метрик** в виде bar chart

## Интерактивные возможности

```python
interact(
    interactive_train,
    model_name=Dropdown(["Linear Regression", "Logistic Regression"]),
    lr=FloatSlider(0.001, 0.5, 0.001),
    epochs=IntSlider(100, 2000, 50)
)
```

Позволяет:
- Выбирать модель (линейная/логистическая регрессия)
- Настраивать learning rate (0.001-0.5)
- Настраивать количество эпох (100-2000)
- Видеть изменения в реальном времени

## Анализ признаков

### Самые важные признаки:
1. **median_income** - наибольшее влияние на стоимость
2. **housing_median_age** - возраст домов
3. **rooms_per_household** - созданный признак

### Созданные признаки:
- rooms_per_household = total_rooms / households
- bedrooms_per_room = total_bedrooms / total_rooms
- population_per_household = population / households

## Выводы

1. **Градиентный спуск** эффективно обучает обе модели
2. **Линейная регрессия** хорошо предсказывает стоимость (R² = 0.65)
3. **Логистическая регрессия** точно классифицирует (Accuracy = 82%)
4. **Регуляризация** помогает предотвратить переобучение
5. **Интерактивный интерфейс** полезен для понимания влияния гиперпараметров
6. **Собственная реализация** показывает результаты, сравнимые с sklearn

## Возможные улучшения

1. Добавить больше методов оптимизации (Adam, RMSprop)
2. Реализовать k-fold кросс-валидацию
3. Добавить feature engineering
4. Реализовать нейронные сети
5. Добавить анализ географических данных (longitude, latitude)

## Использование кода

```python
# Для линейной регрессии
model = LinearRegressionGD(learning_rate=0.01, n_iter=1000)
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# Для логистической регрессии
model = LogisticRegressionGD(learning_rate=0.1, n_iter=500)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)
```
